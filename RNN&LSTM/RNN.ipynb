{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN(Recurrent Neural Network)\n",
    "\n",
    "# torch.nn.RNN(*args, **kwargs) # *args는 튜플, **kargs는 딕셔너리로 처리\n",
    "\n",
    "# Parameters\n",
    "\n",
    "# input_size : 입력 사이즈\n",
    "# hidden_size : 은닉층 사이즈\n",
    "# num_layers : RNN의 은닉층 레이어 갯수\n",
    "# nonlinearity : 비선형 활성화 함수 (tanh, relu), default : tanh\n",
    "# bias : 편향 활성화 여부, default : True\n",
    "# batch_first : True일시 Output_size : (batch,seq,feature), default : False\n",
    "# dropout : 드롭아웃 비율 설정, default : 0\n",
    "# bidirectional : True=> 양방향 RNN, default : False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 7]\n",
      "[6, 4]\n",
      "[6, 1]\n",
      "[3, 7]\n",
      "[3, 4]\n",
      "[3, 1]\n",
      "Epoch: 0100 cost = 0.387569\n",
      "Epoch: 0200 cost = 0.075454\n",
      "Epoch: 0300 cost = 0.030855\n",
      "Epoch: 0400 cost = 0.017526\n",
      "Epoch: 0500 cost = 0.011607\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "i like dog\n",
      "i love coffee\n",
      "i hate milk\n",
      "you like cat\n",
      "you love milk\n",
      "you hate coffee\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "sentences = ['i like dog', 'i love coffee','i hate milk', \"you like cat\", \"you love milk\", \"you hate coffee\"]\n",
    "\n",
    "# 전처리\n",
    "\n",
    "a = \" \".join(sentences)\n",
    "word_list  = a.split()  # 단어별 토크나이즈 \n",
    "word_set = set(word_list)       # set로 바꿔서 중복 없애기\n",
    "word2id = {w:i for i,w in enumerate(word_set)}  # 딕셔너리 => 워드 : 워드ID\n",
    "id2word = {i:w for i,w in enumerate(word_set)}  # 딕서너리 => 워드ID : 워드\n",
    "n_class = len(word2id)    # 총 단어수 \n",
    "\n",
    "\n",
    "# RNN 파라미터\n",
    "\n",
    "batch_size = len(sentences)\n",
    "# n_step = 2 # 학습하려고 하는 문장 길이 -1   # 지금 문장길이 : 3\n",
    "n_hidden = 5    # 은닉층 레이어 개수\n",
    "\n",
    "def make_batch(sentences):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        word = sentence.split()\n",
    "        input_id = [word2id[n] for n in word[:-1]]   # 마지막 글자는 뺀다.\n",
    "        target_id = word2id[word[-1]]    # target은 마지막 단어\n",
    "        print(input_id)\n",
    "        input_batch.append(np.eye(n_class)[input_id])\n",
    "        target_batch.append(target_id) # 문장별 마지막 단어만 넣는다.\n",
    "        \n",
    "    return input_batch, target_batch\n",
    "\n",
    "input_batch, target_batch = make_batch(sentences)\n",
    "\n",
    "# torch.tensor는 되지만, torch.Tensor는 안 된다.\n",
    "input_batch = torch.tensor(input_batch, dtype=torch.float, requires_grad=True)  # float형만 requires_grad가 가능하다.\n",
    "target_batch = torch.tensor(target_batch, dtype=torch.int64)    # list -> tensor 형 변환, long type 이여야 하기때문에 int64\n",
    "# print(input_batch)\n",
    "\n",
    "# RNN\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):    # Linear라면 사비에르 초기화, 편향 :0\n",
    "        nn.init.xavier_normal_(m.weight)    # m.weight.data\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "nonlinearity = torch.nn.Tanh()\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextRNN,self).__init__()\n",
    "        # RNN input은 원핫 벡터로 어떠한 단어가 들어오는지 알려준다.\n",
    "        self.rnn = nn.RNN(input_size=n_class, hidden_size=n_hidden,dropout = 0)   # dropout >0 : num_layers는 1보다 큰걸 권장한다.\n",
    "        self.layer = nn.Linear(n_hidden,n_class)\n",
    "        # self.softmax = nn.Softmax(dim=1)    # 분류이므로 softmax 계층이 필요하다.\n",
    "        self.layer.apply(init_weights)  # layer계층에 적용시킨다.\n",
    "\n",
    "                \n",
    "    def forward(self,hidden,x):\n",
    "        x = x.transpose(0,1)       # x: N,L,H -> L,N,H => batch_first=False일 때 쓰는 꼴 // True일 때는 N,L,H\n",
    "        outputs, hidden = self.rnn(x,hidden)\n",
    "        output = outputs[-1]    # output의 hidden layer만 필요하다.\n",
    "        y = self.layer(output)  # 클래스 분류\n",
    "        return y\n",
    "    \n",
    "    \n",
    "# Trainging\n",
    "\n",
    "model = TextRNN()\n",
    "criterion = nn.CrossEntropyLoss()   # 분류이므로 (소프트맥스까지 취한다.)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(500):\n",
    "    hidden = torch.zeros(1,batch_size,n_hidden,requires_grad=True)  # hidden 초기화\n",
    "    pred = model(hidden,input_batch)\n",
    "    loss = criterion(pred,target_batch) \n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(loss))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# test\n",
    "input = [sentence.split()[:2] for sentence in sentences]    # 2단어까지만 넣기\n",
    "test_data = [[word2id[i], word2id[j]] for i,j in input] \n",
    "test_list = []\n",
    "for data in test_data:\n",
    "    test_list.append(np.eye(n_class)[list(data)])   # data를 원핫인코딩\n",
    "\n",
    "test_list = torch.tensor(test_list, dtype=torch.float)  # list -> tensor\n",
    "print(test_list)\n",
    "# test이므로 requires_grad=False로 둔다.\n",
    "model.eval()\n",
    "hidden = torch.zeros(1,batch_size,n_hidden,dtype=torch.float,requires_grad=False)\n",
    "pred = model(hidden,test_list) # model의 인풋은 원핫인코딩과 초기화된 hidden이다.\n",
    "_, pred_id = torch.max(pred,1)\n",
    "pred_id = pred_id.tolist()\n",
    "pred_words = [id2word[id] for id in pred_id]\n",
    "for sentence, pred_word in zip(input, pred_words):\n",
    "    print(sentence[0],sentence[1],pred_word)\n",
    "# why? x.transpos(0,1) \n",
    "# outputs.shape가 2,6,2 일까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "tensor(0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5204\\3671581140.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0maa\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maa\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: tensor(0)"
     ]
    }
   ],
   "source": [
    "# 딕셔너리에 tensor는 들어가지 않는다.\n",
    "dic = {0:1, 1:2}\n",
    "a = torch.tensor([0,1])\n",
    "for aa in a:\n",
    "    print(dic[aa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0.3000, 0.3000]),\n",
       "indices=tensor([0, 1]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[0.3,0.1],[0.1,0.3]])\n",
    "torch.max(a,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i like a dog i love coffee i hate milk you like cat you love milk you hate coffee\n",
      "['i', 'like', 'a', 'dog', 'i', 'love', 'coffee', 'i', 'hate', 'milk', 'you', 'like', 'cat', 'you', 'love', 'milk', 'you', 'hate', 'coffee']\n",
      "{'i': 7, 'like': 11, 'a': 2, 'dog': 3, 'love': 14, 'coffee': 18, 'hate': 17, 'milk': 15, 'you': 16, 'cat': 12} {0: 'i', 1: 'like', 2: 'a', 3: 'dog', 4: 'i', 5: 'love', 6: 'coffee', 7: 'i', 8: 'hate', 9: 'milk', 10: 'you', 11: 'like', 12: 'cat', 13: 'you', 14: 'love', 15: 'milk', 16: 'you', 17: 'hate', 18: 'coffee'}\n"
     ]
    }
   ],
   "source": [
    "sentences = ['i like a dog', 'i love coffee','i hate milk', \"you like cat\", \"you love milk\", \"you hate coffee\"]\n",
    "print(\" \".join(sentences)) #리스트안의 문자열 통합\n",
    "a = \" \".join(sentences)\n",
    "print(a.split())    # 띄어쓰기 기준 리스트화 // 단어별토크나이즈\n",
    "word_list  = a.split()\n",
    "word_dict = {w: i for i,w in enumerate(word_list)}  # 딕셔너리 => 워드 : 워드ID\n",
    "number_dict = {i:w for i,w in enumerate(word_list)}  # 딕서너리 => 워드ID : 워드\n",
    "print(word_dict,number_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DILAB\\anaconda3\\envs\\DL\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3501, -1.0705,  0.3398, -0.5640, -0.1408],\n",
       "        [-1.4608, -0.0841,  0.2765, -0.4620, -0.0844],\n",
       "        [ 0.2074, -0.1847,  0.4997, -0.9865, -0.2218]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.randn(3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 3, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = torch.randint(5,(5,))\n",
    "print(a)\n",
    "np.eye(5)[a]    # 원핫벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', 'like'], ['i', 'love'], ['i', 'hate'], ['you', 'like'], ['you', 'love'], ['you', 'hate']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[7, 11], [7, 14], [7, 17], [16, 11], [16, 14], [16, 17]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = ['i like a dog', 'i love coffee','i hate milk', \"you like cat\", \"you love milk\", \"you hate coffee\"]\n",
    "word2id = {w: i for i,w in enumerate(word_list)}  # 딕셔너리 => 워드 : 워드ID\n",
    "id2word = {i:w for i,w in enumerate(word_list)}  # 딕서너리 => 워드ID : 워드\n",
    "input = [sentence.split()[:2] for sentence in sentences]    # 2단어까지만 넣기\n",
    "\n",
    "print(input)\n",
    "test_data = [[word2id[i], word2id[j]] for i,j in input] \n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(5)[torch.tensor([1,3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(5)[[1,3]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c31732501746658df903fde8a57fa60585d40606604d6d9f03f663b5e913695"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
